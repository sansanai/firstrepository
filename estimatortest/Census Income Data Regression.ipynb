{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Census Income Data\n",
    "#  이 데이터는 미국 인구총조사를 통해 조사된 사람에 대해 여러가지 정보들(예, 나이, 교육, 직업 등)과\n",
    "# 수입(Income)이 기록되어 있다. 이 Census Income Data의 여러 features를 이용해서 이 사람의 수입이 \n",
    "# $50,000 을 넘을지를  넘지못할지를 예측하는 이진분류기 모델을 만드는 것이다.\n",
    "# 이진 분류기를 만들기 위해 logistic Regreesion을 사용한다. \n",
    "# Census Income Data의 features들은 연속값 과 이진값으로 나누어진다. 전체 특징들은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd77ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번 강의에서 우이른 이진 분류 문제를 성별, 교육, 그리고 직업 등의 특성들에 관한 인구조사 데이터를 \n",
    "# 기반으로 한사람의 연봉이 $50,000 이 넘는지를 판단하려고 한다. 로지스틱 회귀 모델을 주어진 개인 정보를\n",
    "# 가지고 교육할 것이고, 모델은 개인의 연봉이 $50,000 이상일 가능성으로 해석될 수 있는 0과 1사이 숫자를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인구 조사 데이터 에 대한 컬럼 설명 .. census income data column....\n",
    "Column Namne     type       Description\n",
    "age             Continuous   The age of the individual\n",
    "workclass       Categorical  The type of employer the individual has (government, miliatary, private 등)\n",
    "fnlwgt          Continuous   The number of people the census takers believe that observation represents (sample weight). final weight will  not be used.\n",
    "education       Categorical  The highest level of education achieved for that individual.\n",
    "education_num   Continuous   The highest level of education in numerical form\n",
    "marital_status  Categorical  Marital status of the individual 혼인상태\n",
    "occupation      Categorical  The occupation of the individual\n",
    "relationship    Categorical  Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n",
    "race            Categorical  Amer-Indican-Eskimo, Asian-Pac-Islander, Black, White, Other\n",
    "gender          Categorical  Female, Male.\n",
    "capital_gain    Continuous   Captial gain recorded\n",
    "captial_loss    Continuous   Captial losses recorded\n",
    "hours_per_week  Continuous   Hours worked per week\n",
    "native_country  Categorical  Country of origin of the individual\n",
    "income_bracket   Caetgorical  '>50k' or '<=50k', meaning whether the person makes more than $50,000 annually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c9791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data는 다음 폴더에 있다. \n",
    "# https://archive.ics.uci.edu/ml/machine-learning-databases/adult/\n",
    "# wget 으로 다운받아서 ./data/census_income/ 에 adult.data,  adult.names, adutl.test를 다운로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4fb924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.estimator API를 이용한 TensorFlow Wide & Deep Tutorial 예제..\n",
    "#\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags = tf.app.flags\n",
    "# FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd48238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_wide_deep_flags(flags):\n",
    "    ''' model type 과 학습을 위한 flag들을 지정합니다.'''\n",
    "#     flags_core.define_base()\n",
    "#     flags_core.define_benchmark()\n",
    "    \n",
    "# #     flags.adopt_module_key_flags(flags_core)\n",
    "    flags = tf.app.flags\n",
    "#     FLAGS = flags.FLAGS\n",
    "    \n",
    "    flags.DEFINE_string('model_type', 'wide_n_deep', \"valid model types: {'wide','deep', 'wide_n_deep'}.\")\n",
    "    flags.DEFINE_string('data_dir', './data/census_income/', 'path to data directory')\n",
    "    flags.DEFINE_string('model_dir', './model/census_model/', 'path to base dirctory for output models.')\n",
    "    flags.DEFINE_integer('train_steps', 200, 'number of training steps')\n",
    "    flags.DEFINE_integer('epochs_between_evals', 2, 'number of epochs betwen evaluations')\n",
    "    flags.DEFINE_integer('batch_size', 40, ' batch size for training or evaluation. must divide evenly into the data sizes.')\n",
    "    flags.DEFINE_string('f', '', 'kernel') # unknown command line flag 'f' 오류 수정..\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687a6cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 햑습에 사용할 컬럼들을 정의한다.\n",
    "columns=[\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num', \n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket']\n",
    "label_column = 'label'\n",
    "\n",
    "categorical_columns = ['workclass', 'education', 'marital_status', 'occupation', 'relationship',\n",
    "                      'race', 'gender', 'native_country']\n",
    "continuous_columns = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 컬럼들의 기본값을 지정한다.\n",
    "# _csv_column_defatults = [[0], [''], [0], [''], [0],[''],[''],[''],[''],[''],\n",
    "#                          [0], [0],[0], [''],['']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191eb3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed669f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 과 validation 에 사용할 train validation 데이터 갯수를 지정한다.\n",
    "_num_examples = {\n",
    "    'train':32561,\n",
    "    'validation':16281,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffbe0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_prefix = {'wide':'linear/', 'deep':'dnn/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f8244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_wide_deep_flags():\n",
    "#     ''' model type 과 학습을 위한 flag들을 지정합니다.'''\n",
    "# #     flags_core.define_base()\n",
    "# #     flags_core.define_benchmark()\n",
    "    \n",
    "# #     flags.adopt_module_key_flags(flags_core)\n",
    "#     flags = tf.app.flags\n",
    "#     FLAGS = flags.FLAGS\n",
    "    \n",
    "#     flags.DEFINE_string(model_type, 'wide_deep', 'select model topology')\n",
    "#     flags.DEFINE_string(data_dir, './data/census_income/', 'path to data directory')\n",
    "#     flags.DEFINE_string(model_dir, './model/census_model/', 'path to model directory')\n",
    "#     flags.DEFINE_integer(train_epochs, 40, 'number of epochs')\n",
    "#     flags.DEFINE_integer(epochs_between_evals, 2, 'number of epochs betwen evaluations')\n",
    "#     flags.DEFINE_integer(batch_size, 40, ' batch size for training or evaluation. must divide evenly into the data sizes.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca88ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_columns():\n",
    "    ''' feature columns 를 설정한다. '''\n",
    "    # continuous base column\n",
    "    age = tf.feature_column.numeric_column('age')\n",
    "    education_num = tf.feature_column.numeric_column('education_num')\n",
    "    capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
    "    capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
    "    hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
    "    \n",
    "    # categorical base column\n",
    "    gender = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'gender', ['female', 'male'])\n",
    "    race = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'race', ['Amer-Indian-Eskimo', 'Asian-Pac-Island', 'Black', \n",
    "                 'Other', 'White'])\n",
    "#     education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "#         'education', [\n",
    "#             'Bachelos', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
    "#             'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
    "#             '5th-6th', '10th', '1st-4th', 'Preschool', '12th'\n",
    "#         ])\n",
    "    education = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'education', hash_bucket_size=1000\n",
    "    )\n",
    "#     marital_status =  tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "#         'martial_status', [\n",
    "#             'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "#             'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'\n",
    "#         ]) # 혼인상태.\n",
    "    marital_status = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'marital_status', hash_bucket_size=100)\n",
    "#     relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "#         'relationship', [\n",
    "#             'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
    "#             'Other-relative'\n",
    "#         ])\n",
    "    relationship = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'relationship', hash_bucket_size=100)\n",
    "#     workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "#         'workclass', [\n",
    "#             'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov', \n",
    "#             'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'\n",
    "#         ]\n",
    "#     )\n",
    "    workclass = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'workclass', hash_bucket_size=100)\n",
    "    \n",
    "    # To show an exmaple of hashing:\n",
    "    occupation = tf.feature_column.categorical_column_with_hash_bucket (\n",
    "        'occupation', hash_bucket_size=1000\n",
    "    )\n",
    "    native_country = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'native_country', hash_bucket_size=1000)\n",
    "    \n",
    "    # Transformations.\n",
    "    age_buckets = tf.feature_column.bucketized_column(\n",
    "        age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n",
    "    )\n",
    "    \n",
    "    # Wide columns and deep columns\n",
    "    base_columns = [\n",
    "        gender, native_country, education, occupation, workclass,\n",
    "        marital_status, relationship, age_buckets,\n",
    "    ]\n",
    "    \n",
    "    crossed_columns=[\n",
    "        tf.feature_column.crossed_column(\n",
    "            ['education', 'occupation'], hash_bucket_size=int(1e4)),\n",
    "        tf.feature_column.crossed_column(\n",
    "            [age_buckets, 'education', 'occupation'], hash_bucket_size=int(1e4)),\n",
    "    ]\n",
    "    \n",
    "    wide_columns = base_columns+crossed_columns\n",
    "    \n",
    "#     deep_colums = [\n",
    "#         age,\n",
    "#         education_num,\n",
    "#         capital_gain,\n",
    "#         captial_loss,\n",
    "#         hours_per_week,\n",
    "#         tf.feature_column.indicator_column(workclass),\n",
    "#         tf.feature_column.indicator_column(education),\n",
    "#         tf.feature_column.indicator_column(marital_status),\n",
    "#         tf.feature_column.indicator_column(relationship),\n",
    "#         #To show an example of embedding\n",
    "#         tf.feature_column.embedding_column(occupation, dimension=8),\n",
    "#     ]\n",
    "    deep_columns = [\n",
    "        tf.feature_column.embedding_column(workclass, dimension=8),\n",
    "        tf.feature_column.embedding_column(education, dimension=8),\n",
    "        tf.feature_column.embedding_column(marital_status, dimension=8),\n",
    "        tf.feature_column.embedding_column(gender, dimension=8),\n",
    "        tf.feature_column.embedding_column(relationship, dimension=8),\n",
    "        tf.feature_column.embedding_column(race, dimension=8),\n",
    "        tf.feature_column.embedding_column(native_country, dimension=8),\n",
    "        tf.feature_column.embedding_column(occupation, dimension=8),\n",
    "        age,\n",
    "        education_num, \n",
    "        capital_gain,\n",
    "        capital_loss,\n",
    "        hours_per_week,\n",
    "    ]\n",
    "    \n",
    "    return wide_columns, deep_columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd98770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "    ''' Estimator model type에 따라서 estimator를 설정한다. '''\n",
    "    wide_columns, deep_columns = build_model_columns()\n",
    "#     hidden_units = [100, 75, 50, 25]\n",
    "    hidden_units = [100, 50]\n",
    "    \n",
    "#     # create a tf.estimator.RunConfig to enusre the model is run on CPU, which\n",
    "#     # trains faster than GPU for this model.\n",
    "#     run_config = tf.estimator.RunConfig().replace(\n",
    "#         session_config = tf.ConfigProto(device_count={'GPU': 0 }))\n",
    "    \n",
    "    if model_type == 'wide':\n",
    "        return tf.estimator.LinearClassifier(\n",
    "            model_dir = model_dir,\n",
    "            feature_columns = wide_columns\n",
    "#             , config = run_config\n",
    "        )\n",
    "    elif model_type == 'deep' :\n",
    "        return tf.estimator.DNNClassifier(\n",
    "            model_dir = model_dir,\n",
    "            feature_columns = deep_columns,\n",
    "            hidden_units = hidden_units\n",
    "#             , config = run_config\n",
    "        )\n",
    "    else :\n",
    "        return tf.estimator.DNNLinearCombinedClassifier(\n",
    "            model_dir = model_dir,\n",
    "            linear_feature_columns = wide_columns,\n",
    "            dnn_feature_columns = deep_columns,\n",
    "            dnn_hidden_units = hidden_units\n",
    "#             , config = run_config\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "#     '''Estimator를 위한 input function을 정의한다.'''\n",
    "    \n",
    "#     assert tf.gfile.Exists(data_file), (\n",
    "#         '{} not found. Please make suer you have run data_download.py and '\n",
    "#         'set the --data_dir agrument to the correct path'.format(data_file)\n",
    "#     )\n",
    "    \n",
    "#     # csv 파일을 파싱한다.\n",
    "#     def parse_csv(value):\n",
    "#         print('Parsing  {}'.format(data_file))\n",
    "#         columns = tf.decode_csv(value, record_defaults = _csv_column_defaults)\n",
    "#         features = dict(zip(_csv_columns, cloumns))\n",
    "#         labels = features.pop('incomen_bracket')\n",
    "#         return features, tf.equal(labels, '>50k')\n",
    "    \n",
    "#     # extract lines from input files using the dataset api\n",
    "#     dataset =  tf.data.TextLineDataset(data_file)\n",
    "    \n",
    "#     if shuffle:\n",
    "#         dataset = dataset.shuffle(buffer_size=_num_examples['train'])\n",
    "        \n",
    "#     dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "    \n",
    "#     # we call repeat after shuffling, rather than before, to prevent separte\n",
    "#     # epcosh from blending together.\n",
    "#     dataset = dataset.repeat(num_epochs)\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "173b19be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "    '''Input builder function'''\n",
    "    # Create a dictionary mapping freom each continuous fature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k:tf.constant(df[k].values)for k in continuous_columns}\n",
    "    \n",
    "    # Create a dictionary mapping from each categorical feature column name (k) to\n",
    "    # the values of that column stored in a tf.SparseTensor.\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "        indices=[[i,0] for i in range(df[k].size)],\n",
    "        values = df[k].values,\n",
    "#         shape=[df[k].size, 1]) for k in categorical_columns}\n",
    "        dense_shape=[df[k].size, 1]) for k in categorical_columns}\n",
    "    \n",
    "    # Merge the tow dictionaries into one\n",
    "#     feature_cols = dict(continuous_cols.items()+categorical_cols.items()) #TypeError: unsupported operand type(s) for +: 'dict_items' and 'dict_items'\n",
    "#     feature_cols = dict(continuous_cols.items()|categorical_cols.items())\n",
    "#     feature_cols = dict(**continuous_cols, **categorical_cols)\\\n",
    "    feature_cols = dict(list(continuous_cols.items())+list(categorical_cols.items()))\n",
    "    # converts the label column  into a constant Tensor\n",
    "    label = tf.constant(df[label_column].values)\n",
    "    \n",
    "    #return the feature columns and the lable\n",
    "    return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef98598",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "define_wide_deep_flags(flags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8551f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dir  : ./data/census_income/\n",
      "train file  : ./data/census_income/adult.data\n",
      "test file  : ./data/census_income/adult.test\n"
     ]
    }
   ],
   "source": [
    "print('data dir  : {}'.format(FLAGS.data_dir))\n",
    "# 파일로 부터 트레이닝 데이터와 테스트 데이터를 읽어온다.\n",
    "train_file = os.path.join(FLAGS.data_dir, 'adult.data')\n",
    "test_file = os.path.join(FLAGS.data_dir, 'adult.test')\n",
    "print('train file  : {}'.format(train_file))\n",
    "print('test file  : {}'.format(test_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e7f92b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model directory : ./model/census_model/\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model/census_model/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5de4f1c2d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model/census_model/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 400 into ./model/census_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.21309116, step = 401\n",
      "INFO:tensorflow:global_step/sec: 31.9364\n",
      "INFO:tensorflow:loss = 0.1589894, step = 501 (3.132 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 600 into ./model/census_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.12290118.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-07-05T19:46:45Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model/census_model/model.ckpt-600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2021-07-05-19:46:47\n",
      "INFO:tensorflow:Saving dict for global step 600: accuracy = 1.0, accuracy_baseline = 1.0, auc = 1.0, auc_precision_recall = 0.0, average_loss = 3.975016e-06, global_step = 600, label/mean = 0.0, loss = 0.06471724, precision = 0.0, prediction/mean = 3.9709757e-06, recall = 0.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 600: ./model/census_model/model.ckpt-600\n",
      " accuracy : 1.0\n",
      " accuracy_baseline : 1.0\n",
      " auc : 1.0\n",
      " auc_precision_recall : 0.0\n",
      " average_loss : 3.9750161704432685e-06\n",
      " label/mean : 0.0\n",
      " loss : 0.06471724063158035\n",
      " precision : 0.0\n",
      " prediction/mean : 3.970975740230642e-06\n",
      " recall : 0.0\n",
      " global_step : 600\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_file, names=columns, skipinitialspace=True)\n",
    "df_test = pd.read_csv(test_file, names=columns, skipinitialspace=True, skiprows=1)\n",
    "# test_file에 첫라인은 실데이터가 아니다.\n",
    "\n",
    "df_train[label_column]= (df_train['income_bracket'].apply(lambda x: \">50k\" in x)).astype(int)\n",
    "df_test[label_column] = (df_test['income_bracket'].apply(lambda x: \">50k\" in x)).astype(int)\n",
    "\n",
    "model_dir = FLAGS.model_dir\n",
    "print('model directory : {}'.format(model_dir))\n",
    "\n",
    "model = build_estimator(FLAGS.model_dir, FLAGS.model_type)\n",
    "model.train(input_fn = lambda : input_fn(df_train), steps=FLAGS.train_steps)\n",
    "results = model.evaluate(input_fn = lambda : input_fn(df_test), steps=1)\n",
    "\n",
    "for key in results:\n",
    "    print(\" {} : {}\".format(key, results[key]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c245a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def export_model(model, model_type, export_dir):\n",
    "#     '''SaveModel format으로 export 한다\n",
    "#     Args:\n",
    "#         model : Estimator object\n",
    "#         model_type :  model type을 나타내는 string. 예: 'wide', 'deep' 혹은 'wide_deep'\n",
    "#         export_dir : model을 export 할 폴더 경로\n",
    "#     '''\n",
    "#     wide_columns, deep_columns = build_model_columns()\n",
    "    \n",
    "#     if model_type == 'wide':\n",
    "#         columns = wide_columns\n",
    "#     elif model_type == 'deep':\n",
    "#         columns = deep_columns\n",
    "#     else :\n",
    "#         columns = wide_columns + deep_columns\n",
    "        \n",
    "#     feature_spec = tf.feature_column.make_parse_example_spec(columns)\n",
    "#     example_input_fn = (\n",
    "#         tf.estimator.export.build_parsing_serving_input_receiver_Fn(feature_spec))\n",
    "#     model.export_savedmodel(export_dir, exmaple_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9bcacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_wide_deep(flags_obj):\n",
    "#     '''\n",
    "#     Wide-Deep training 과 evaluation을 실행한다.\n",
    "#     인자 Arguments:\n",
    "#         flags_obj : parsed 플래그들 (flags)\n",
    "#     '''\n",
    "    \n",
    "#     # model_dir 경로에 파일이 있으면 삭제한다.\n",
    "#     shutil.rmtree(flags_obj.model_dir, ignore_errors = True)\n",
    "#     # tf.estimatore api.를 이용해서 학습 모델을 생성한다.\n",
    "#     model = build_estimator(flags_obj.model_dir, flags_obj.model_type)\n",
    "    \n",
    "#     # 파일로 부터 트레이닝 데이터와 테스트 데이터를 읽어온다.\n",
    "#     train_file = os.path.join(flags_obj.data_dir, 'adult.data')\n",
    "#     test_file = os.path.join(flags_obj.data_dir, 'adult.test')\n",
    "    \n",
    "#     # training을 진행하고, flags.epochs_between_evals epoch 마다 evaluation을 진행한다.\n",
    "#     def trian_input_fn():\n",
    "#         return input_fn(train_file, flags_obj.epochs_between_evals, True, flags_obj.batch_size)\n",
    "    \n",
    "#     def eval_input_fn():\n",
    "#         return input_fn(test_file, 1, False, flags_obj.batch_size)\n",
    "    \n",
    "#     run_params = {\n",
    "#         'batch_size': flags_obj.batch_size,\n",
    "#         'train_epochs' : flags_obj.train_epochs,\n",
    "#         'model_type' : flags_obj.model_type,\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e736144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n",
    "# define_wide_deep_flags()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_7_tf115",
   "language": "python",
   "name": "py3_7_tf115"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
