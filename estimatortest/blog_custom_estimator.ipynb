{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# Distributed_TensorFlow under the License is Distributed_TensorFlow on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# This is the complete code for the following blogpost:\n",
    "# https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import six.moves.urllib.request as request\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Check that we have correct TensorFlow version installed\n",
    "tf_version = tf.__version__\n",
    "tf.logging.info(\"TensorFlow version: {}\".format(tf_version))\n",
    "assert \"1.4\" <= tf_version, \"TensorFlow r1.4 or later is needed\"\n",
    "\n",
    "# Windows users: You only need to change PATH, rest is platform independent\n",
    "# PATH = \"/tmp/tf_custom_estimators\"\n",
    "\n",
    "# Fetch and store Training and Test dataset files\n",
    "# PATH_DATASET = PATH + os.sep + \"dataset\"\n",
    "# FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TRAIN = \"./data/iris_training.csv\"\n",
    "# FILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "FILE_TEST = \"./data/iris_test.csv\"\n",
    "# URL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "# URL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "# def downloadDataset(url, file):\n",
    "#     if not os.path.exists(PATH_DATASET):\n",
    "#         os.makedirs(PATH_DATASET)\n",
    "#     if not os.path.exists(file):\n",
    "#         data = request.urlopen(url).read()\n",
    "#         with open(file, \"wb\") as f:\n",
    "#             f.write(data)\n",
    "#             f.close()\n",
    "# downloadDataset(URL_TRAIN, FILE_TRAIN)\n",
    "# downloadDataset(URL_TEST, FILE_TEST)\n",
    "\n",
    "# The CSV features in our training & test data\n",
    "feature_names = [\n",
    "    'SepalLength',\n",
    "    'SepalWidth',\n",
    "    'PetalLength',\n",
    "    'PetalWidth']\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def my_input_fn(file_path, repeat_count=1, shuffle_count=1):\n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "        label = parsed_line[-1]  # Last element is the label\n",
    "        del parsed_line[-1]  # Delete last element\n",
    "        features = parsed_line  # Everything but last elements are the features\n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d\n",
    "\n",
    "    dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n",
    "        .skip(1)  # Skip header row\n",
    "        .map(decode_csv, num_parallel_calls=4)  # Decode each line\n",
    "        .cache() # Warning: Caches entire dataset, can cause out of memory\n",
    "        .shuffle(shuffle_count)  # Randomize elems (1 == no operation)\n",
    "        .repeat(repeat_count)    # Repeats dataset this # times\n",
    "        .batch(32)\n",
    "        .prefetch(1)  # Make sure you always have 1 batch ready to serve\n",
    "    )\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "def my_model_fn(\n",
    "    features, # This is batch_features from input_fn\n",
    "    labels,   # This is batch_labels from input_fn\n",
    "    mode):    # And instance of tf.estimator.ModeKeys, see below\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        tf.logging.info(\"my_model_fn: PREDICT, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        tf.logging.info(\"my_model_fn: EVAL, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        tf.logging.info(\"my_model_fn: TRAIN, {}\".format(mode))\n",
    "\n",
    "    # All our inputs are feature columns of type numeric_column\n",
    "    feature_columns = [\n",
    "        tf.feature_column.numeric_column(feature_names[0]),\n",
    "        tf.feature_column.numeric_column(feature_names[1]),\n",
    "        tf.feature_column.numeric_column(feature_names[2]),\n",
    "        tf.feature_column.numeric_column(feature_names[3])\n",
    "    ]\n",
    "\n",
    "    # Create the layer of input\n",
    "    input_layer = tf.feature_column.input_layer(features, feature_columns)\n",
    "\n",
    "    # Definition of hidden layer: h1\n",
    "    # We implement it as a fully-connected layer (tf.layers.dense)\n",
    "    # Has 10 neurons, and uses ReLU as the activation function\n",
    "    # Takes input_layer as input\n",
    "    h1 = tf.layers.Dense(10, activation=tf.nn.relu)(input_layer)\n",
    "\n",
    "    # Definition of hidden layer: h2 (this is the logits layer)\n",
    "    # Similar to h1, but takes h1 as input\n",
    "    h2 = tf.layers.Dense(10, activation=tf.nn.relu)(h1)\n",
    "\n",
    "    # Output 'logits' layer is three number = probability distribution\n",
    "    # between Iris Setosa, Versicolor, and Viginica\n",
    "    logits = tf.layers.Dense(3)(h2)\n",
    "\n",
    "    # class_ids will be the model prediction for the class (Iris flower type)\n",
    "    # The output node with the highest value is our prediction\n",
    "    predictions = { 'class_ids': tf.argmax(input=logits, axis=1) }\n",
    "\n",
    "    # 1. Prediction mode\n",
    "    # Return our prediction\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Evaluation and Training mode\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Calculate the accuracy between the true labels, and our predictions\n",
    "    accuracy = tf.metrics.accuracy(labels, predictions['class_ids'])\n",
    "\n",
    "    # 2. Evaluation mode\n",
    "    # Return our loss (which is used to evaluate our model)\n",
    "    # Set the TensorBoard scalar my_accurace to the accuracy\n",
    "    # Obs: This function only sets value during mode == ModeKeys.EVAL\n",
    "    # To set values during training, see tf.summary.scalar\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops={'my_accuracy': accuracy})\n",
    "\n",
    "    # If mode is not PREDICT nor EVAL, then we must be in TRAIN\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN, \"TRAIN is only ModeKey left\"\n",
    "\n",
    "    # 3. Training mode\n",
    "\n",
    "    # Default optimizer for DNNClassifier: Adagrad with learning rate=0.05\n",
    "    # Our objective (train_op) is to minimize loss\n",
    "    # Provide global step counter (used to count gradient updates)\n",
    "    optimizer = tf.train.AdagradOptimizer(0.05)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "\n",
    "    # Set the TensorBoard scalar my_accuracy to the accuracy\n",
    "    # Obs: This function only sets the value during mode == ModeKeys.TRAIN\n",
    "    # To set values during evaluation, see eval_metrics_ops\n",
    "    tf.summary.scalar('my_accuracy', accuracy[1])\n",
    "\n",
    "    # Return training operations: loss and train_op\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode,\n",
    "        loss=loss,\n",
    "        train_op=train_op)\n",
    "\n",
    "# Create a custom estimator using my_model_fn to define the model\n",
    "tf.logging.info(\"Before classifier construction\")\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=my_model_fn,\n",
    "    model_dir=PATH)  # Path to where checkpoints etc are stored\n",
    "tf.logging.info(\"...done constructing classifier\")\n",
    "\n",
    "# 500 epochs = 500 * 120 records [60000] = (500 * 120) / 32 batches = 1875 batches\n",
    "# 4 epochs = 4 * 30 records = (4 * 30) / 32 batches = 3.75 batches\n",
    "\n",
    "# Train our model, use the previously function my_input_fn\n",
    "# Input to training is a file with training example\n",
    "# Stop training after 8 iterations of train data (epochs)\n",
    "tf.logging.info(\"Before classifier.train\")\n",
    "classifier.train(\n",
    "    input_fn=lambda: my_input_fn(FILE_TRAIN, 500, 256))\n",
    "tf.logging.info(\"...done classifier.train\")\n",
    "\n",
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics such as: loss & average_loss\n",
    "tf.logging.info(\"Before classifier.evaluate\")\n",
    "evaluate_result = classifier.evaluate(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, 4))\n",
    "tf.logging.info(\"...done classifier.evaluate\")\n",
    "tf.logging.info(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    tf.logging.info(\"   {}, was: {}\".format(key, evaluate_result[key]))\n",
    "\n",
    "# Predict the type of some Iris flowers.\n",
    "# Let's predict the examples in FILE_TEST, repeat only once.\n",
    "predict_results = classifier.predict(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, 1))\n",
    "tf.logging.info(\"Prediction on test file\")\n",
    "for prediction in predict_results:\n",
    "    # Will print the predicted class, i.e: 0, 1, or 2 if the prediction\n",
    "    # is Iris Setosa, Vericolor, Virginica, respectively.\n",
    "    tf.logging.info(\"...{}\".format(prediction[\"class_ids\"]))\n",
    "\n",
    "# Let create a dataset for prediction\n",
    "# We've taken the first 3 examples in FILE_TEST\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Setosa\n",
    "\n",
    "def new_input_fn():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4)  # Need to split into our 4 features\n",
    "        return dict(zip(feature_names, x))  # To build a dict of them\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None  # In prediction, we have no labels\n",
    "\n",
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)\n",
    "\n",
    "# Print results\n",
    "tf.logging.info(\"Predictions on memory\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"]  # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        tf.logging.info(\"...I think: {}, is Iris Setosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        tf.logging.info(\"...I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        tf.logging.info(\"...I think: {}, is Iris Virginica\".format(prediction_input[idx]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_7_tf115",
   "language": "python",
   "name": "py3_7_tf115"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
